# Notas Completas: Tuning Llama 3 Parameters

## üìö Curso: Working with Llama 3
**Lecci√≥n:** Tuning Llama 3 Parameters  
**XP de la lecci√≥n:** 50 XP  
**XP del d√≠a:** 750 XP

---

## üéØ 1. Introducci√≥n: ¬øPor qu√© ajustar par√°metros?

En la lecci√≥n anterior aprendimos a generar respuestas b√°sicas con Llama 3. Ahora surge la pregunta:

**¬øC√≥mo controlamos la calidad, aleatoriedad y longitud de las respuestas?**

### Caso de uso pr√°ctico:

Imagina que est√°s generando **descripciones de productos para diferentes sitios de e-commerce**:

- **Algunos necesitan ser:**
  - Factuales y concisos
  - Directos al punto
  - Profesionales y t√©cnicos

- **Otros deben ser:**
  - Atractivos y creativos
  - Persuasivos y emocionantes
  - Descriptivos y detallados

**Soluci√≥n:** Ajustar los par√°metros de decodificaci√≥n de Llama 3 para adaptar el tono y estilo seg√∫n las necesidades.

---

## üîß 2. ¬øQu√© son los par√°metros de decodificaci√≥n?

### Definici√≥n:

Los **par√°metros de decodificaci√≥n** (decoding parameters) son configuraciones que:

- **"Decodifican"** o transforman la salida cruda del modelo en texto legible
- Permiten **modificar las respuestas** manteniendo el contenido central
- Controlan **c√≥mo el modelo selecciona las palabras** para generar la respuesta

### Analog√≠a:

Piensa en el modelo como un m√∫sico:
- El **contenido** es la melod√≠a base (siempre la misma)
- Los **par√°metros** son el estilo de interpretaci√≥n (jazz, cl√°sico, rock)
- El resultado final cambia seg√∫n c√≥mo "interpretes" la melod√≠a

---

## üìä 3. Los 4 par√°metros clave de Llama 3

Exploraremos cuatro par√°metros fundamentales:

| Par√°metro | Funci√≥n | Controla |
|-----------|---------|----------|
| **`temperature`** | Controla la aleatoriedad | Qu√© tan predecible o creativo es el texto |
| **`top_k`** | Limita selecci√≥n de tokens | Cu√°ntas palabras considera el modelo en cada paso |
| **`top_p`** | Ajusta selecci√≥n por probabilidad | Qu√© porcentaje de probabilidad acumulada usar |
| **`max_tokens`** | Limita longitud de respuesta | Cu√°ntos tokens (palabras) generar como m√°ximo |

---

## üå°Ô∏è 4. Temperature (Temperatura)

### ¬øQu√© es?

La **temperatura** controla la **aleatoriedad** o **creatividad** de las respuestas del modelo.

### Rango de valores:

- **T√≠picamente:** 0.0 a 1.0
- **Valores extremos:** Pueden ir hasta 2.0, pero no es com√∫n

### Comportamiento:

#### üßä Temperatura BAJA (cercana a 0)

**Valor:** 0.0 - 0.3

**Caracter√≠sticas:**
- ‚úÖ Respuestas **predecibles** y **consistentes**
- ‚úÖ M√°s **factuales** y **precisas**
- ‚úÖ Menos variaci√≥n entre generaciones
- ‚ùå Puede ser **repetitivo** o **aburrido**
- ‚ùå Menos **creatividad**

**Ejemplo - Descripci√≥n de smartwatch con temperature=0.1:**

```
This smartwatch features:
- Heart rate monitor
- GPS tracking
- Long battery life (up to 7 days)
- Water resistance (50m)
- Sleep tracking
```

**Uso recomendado:**
- Documentaci√≥n t√©cnica
- Respuestas factuales
- An√°lisis de datos
- Res√∫menes precisos
- Traducciones

---

#### üî• Temperatura ALTA (cercana a 1)

**Valor:** 0.7 - 1.0

**Caracter√≠sticas:**
- ‚úÖ Respuestas **creativas** y **variadas**
- ‚úÖ M√°s **expresivas** y **√∫nicas**
- ‚úÖ Mayor **diversidad** en el lenguaje
- ‚ùå Puede ser **menos preciso**
- ‚ùå A veces **incoherente** (si es muy alta)

**Ejemplo - Descripci√≥n de smartwatch con temperature=0.9:**

```
Discover the ultimate companion for your active lifestyle! 
This cutting-edge smartwatch seamlessly blends style with 
functionality, featuring an advanced heart rate monitor that 
keeps you in tune with your body's rhythms. Navigate your 
adventures with precision GPS, while the remarkable 7-day 
battery life ensures you're always connected. Dive into life 
with confidence thanks to its impressive 50m water resistance!
```

**Uso recomendado:**
- Marketing y copywriting
- Contenido creativo
- Storytelling
- Brainstorming de ideas
- Generaci√≥n de variaciones

---

### üìù C√≥digo de ejemplo:

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/llama-3-8b.gguf")

prompt = "Write a product description for a smartwatch."

# Respuesta predecible (baja temperatura)
response_low = llm(
    prompt=prompt,
    temperature=0.1,
    max_tokens=100
)

# Respuesta creativa (alta temperatura)
response_high = llm(
    prompt=prompt,
    temperature=0.9,
    max_tokens=100
)

print("LOW TEMPERATURE (0.1):")
print(response_low["choices"][0]["text"])
print("\n" + "="*60 + "\n")
print("HIGH TEMPERATURE (0.9):")
print(response_high["choices"][0]["text"])
```

---

## üé≤ 5. Top-k

### ¬øQu√© es?

**Top-k** controla **cu√°ntas palabras** (tokens) el modelo considera usar **cada vez que a√±ade una nueva palabra** a la respuesta.

### C√≥mo funciona:

En cada paso de generaci√≥n, el modelo:
1. Calcula probabilidades para todas las palabras posibles
2. Ordena las palabras por probabilidad (de mayor a menor)
3. **Solo considera las k palabras m√°s probables**
4. Selecciona una de esas k palabras

### Valores t√≠picos:

- **Muy bajo:** `top_k = 1` (solo la palabra m√°s probable)
- **Bajo:** `top_k = 5-10`
- **Medio:** `top_k = 20-30`
- **Alto:** `top_k = 50-100`

---

#### üîí Top-k BAJO (ej: top_k=1)

**Caracter√≠sticas:**
- ‚úÖ Respuestas **muy predecibles**
- ‚úÖ **Consistentes** entre generaciones
- ‚ùå Puede ser **repetitivo**
- ‚ùå Falta de **variedad**

**Ejemplo - Smartwatch con top_k=1:**

```
This smartwatch has a heart rate monitor, GPS, and long battery life.
This smartwatch has a heart rate monitor, GPS, and long battery life.
This smartwatch has a heart rate monitor, GPS, and long battery life.
```
*(Nota: Generaciones m√∫ltiples producen casi el mismo resultado)*

**Comportamiento:**
- El modelo **siempre elige la palabra m√°s probable**
- Similar a una lista directa de caracter√≠sticas
- Muy parecido al efecto de temperatura baja

---

#### üé® Top-k ALTO (ej: top_k=50)

**Caracter√≠sticas:**
- ‚úÖ Respuestas **m√°s expresivas**
- ‚úÖ Mayor **variedad** de vocabulario
- ‚úÖ M√°s **natural** y **fluido**
- ‚ö†Ô∏è Puede introducir palabras menos comunes

**Ejemplo - Smartwatch con top_k=50:**

```
Experience cutting-edge wearable technology with this remarkable 
smartwatch, boasting comprehensive health monitoring through its 
sophisticated heart rate sensor, precise GPS navigation for your 
outdoor adventures, and an exceptional battery that lasts up to 
a full week on a single charge.
```

**Comportamiento:**
- El modelo puede elegir entre las **50 palabras m√°s probables**
- Permite sin√≥nimos y expresiones variadas
- M√°s libertad creativa manteniendo coherencia

---

### üìù C√≥digo de ejemplo:

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/llama-3-8b.gguf")

prompt = "Write a product description for a smartwatch."

# Respuesta muy predecible (top_k bajo)
response_low_k = llm(
    prompt=prompt,
    top_k=1,
    temperature=0.7,  # Temperatura neutral
    max_tokens=100
)

# Respuesta m√°s expresiva (top_k alto)
response_high_k = llm(
    prompt=prompt,
    top_k=50,
    temperature=0.7,  # Misma temperatura
    max_tokens=100
)

print("LOW TOP-K (1):")
print(response_low_k["choices"][0]["text"])
print("\n" + "="*60 + "\n")
print("HIGH TOP-K (50):")
print(response_high_k["choices"][0]["text"])
```

---

## üìà 6. Top-p (Nucleus Sampling)

### ¬øQu√© es?

**Top-p** es similar a top-k, pero en lugar de limitar por **n√∫mero de palabras**, limita por **probabilidad acumulada**.

Tambi√©n se conoce como **"nucleus sampling"** (muestreo del n√∫cleo).

### C√≥mo funciona:

1. El modelo calcula probabilidades para todas las palabras posibles
2. Ordena las palabras por probabilidad (de mayor a menor)
3. **Suma las probabilidades acumulativamente** hasta alcanzar el valor de `top_p`
4. Solo considera las palabras dentro de ese "n√∫cleo" de probabilidad
5. Selecciona una palabra de ese conjunto

### Valores t√≠picos:

- **Muy bajo:** `top_p = 0.1` (solo las palabras m√°s probables)
- **Bajo:** `top_p = 0.3-0.5`
- **Medio:** `top_p = 0.7-0.8`
- **Alto:** `top_p = 0.9-0.95`
- **M√°ximo:** `top_p = 1.0` (considera todas las palabras)

---

#### üéØ Top-p BAJO (ej: top_p=0.4)

**Caracter√≠sticas:**
- ‚úÖ Salida **enfocada** y **precisa**
- ‚úÖ Solo menciona **funcionalidades centrales**
- ‚úÖ M√°s **conservador** en la selecci√≥n de palabras
- ‚ùå Menos **variedad** l√©xica

**Ejemplo - Smartwatch con top_p=0.4:**

```
This smartwatch includes heart rate monitoring, GPS tracking, 
and extended battery life.
```

**Comportamiento:**
- Solo considera palabras que suman hasta el **40% de probabilidad**
- T√≠picamente son las palabras m√°s obvias y directas
- Respuesta concisa y al grano

---

#### üåà Top-p ALTO (ej: top_p=0.9)

**Caracter√≠sticas:**
- ‚úÖ Respuestas **m√°s variadas**
- ‚úÖ Lista **m√∫ltiples caracter√≠sticas**
- ‚úÖ Lenguaje m√°s **rico** y **descriptivo**
- ‚ö†Ô∏è Puede incluir detalles menos relevantes

**Ejemplo - Smartwatch con top_p=0.9:**

```
Elevate your fitness journey with this feature-packed smartwatch! 
It offers comprehensive health tracking including heart rate 
monitoring, advanced GPS navigation for precise route tracking, 
an impressive 7-day battery life, water resistance up to 50 meters, 
sleep quality analysis, step counting, calorie tracking, and 
smartphone notifications to keep you connected throughout your day.
```

**Comportamiento:**
- Considera palabras que suman hasta el **90% de probabilidad**
- Incluye m√°s opciones de vocabulario
- Permite descripciones m√°s completas y detalladas

---

### üÜö Top-k vs Top-p: ¬øCu√°l usar?

| Aspecto | Top-k | Top-p |
|---------|-------|-------|
| **Criterio** | N√∫mero fijo de palabras | Probabilidad acumulada |
| **Flexibilidad** | Siempre considera k palabras | N√∫mero variable seg√∫n distribuci√≥n |
| **Mejor para** | Control preciso del vocabulario | Adaptaci√≥n din√°mica al contexto |
| **Recomendaci√≥n** | Usar cuando quieres l√≠mite estricto | Usar para m√°s naturalidad |

**üí° Consejo:** Muchos practicionantes usan **ambos juntos** para un control m√°s fino:

```python
response = llm(
    prompt=prompt,
    top_k=40,      # M√°ximo 40 palabras a considerar
    top_p=0.9,     # Pero solo hasta 90% de probabilidad
    temperature=0.7
)
```

---

### üìù C√≥digo de ejemplo:

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/llama-3-8b.gguf")

prompt = "Write a product description for a smartwatch."

# Respuesta enfocada (top_p bajo)
response_low_p = llm(
    prompt=prompt,
    top_p=0.4,
    temperature=0.7,
    max_tokens=100
)

# Respuesta variada (top_p alto)
response_high_p = llm(
    prompt=prompt,
    top_p=0.9,
    temperature=0.7,
    max_tokens=100
)

print("LOW TOP-P (0.4):")
print(response_low_p["choices"][0]["text"])
print("\n" + "="*60 + "\n")
print("HIGH TOP-P (0.9):")
print(response_high_p["choices"][0]["text"])
```

---

## üìè 7. Max Tokens

### ¬øQu√© es?

**Max tokens** limita la **longitud de la respuesta** especificando el n√∫mero m√°ximo de tokens (unidades de palabras) que el modelo puede generar.

### ¬øQu√© es un token?

- Un **token** puede ser:
  - Una palabra completa: `"smartwatch"` = 1 token
  - Parte de una palabra: `"smart"` + `"watch"` = 2 tokens
  - Un s√≠mbolo: `"!"` = 1 token
  - Un espacio o puntuaci√≥n

**Regla aproximada:**
- **En ingl√©s:** 1 token ‚âà 0.75 palabras
- **100 tokens** ‚âà 75 palabras
- **256 tokens** ‚âà 190 palabras

### Valores t√≠picos:

- **Muy corto:** `max_tokens = 10-20` (una oraci√≥n)
- **Corto:** `max_tokens = 50-100` (un p√°rrafo peque√±o)
- **Medio:** `max_tokens = 150-300` (varios p√°rrafos)
- **Largo:** `max_tokens = 500-1000` (respuesta extensa)
- **Muy largo:** `max_tokens = 2000+` (art√≠culo completo)

---

#### üìå Max tokens BAJO (ej: max_tokens=20)

**Uso:** Res√∫menes cortos y precisos

**Ejemplo - Smartwatch con max_tokens=20:**

```
A smartwatch with heart rate monitor, GPS, and long battery life.
```

**Caracter√≠sticas:**
- ‚úÖ **Conciso** y directo
- ‚úÖ Solo informaci√≥n **esencial**
- ‚úÖ R√°pido de generar
- ‚ùå Puede cortar informaci√≥n importante
- ‚ùå Falta de detalles

**Casos de uso:**
- T√≠tulos de productos
- Res√∫menes ejecutivos
- Respuestas r√°pidas
- Etiquetas o tags
- Notificaciones breves

---

#### üìÑ Max tokens ALTO (ej: max_tokens=100-200)

**Uso:** Explicaciones detalladas

**Ejemplo - Smartwatch con max_tokens=100:**

```
Discover the perfect blend of style and functionality with this 
advanced smartwatch. Featuring a state-of-the-art heart rate 
monitor that tracks your cardiovascular health in real-time, 
integrated GPS for accurate navigation and fitness tracking, 
and an impressive battery life that lasts up to 7 days on a 
single charge. Whether you're hitting the gym, exploring the 
outdoors, or managing your daily schedule, this smartwatch is 
your ideal companion for a connected, healthy lifestyle.
```

**Caracter√≠sticas:**
- ‚úÖ **Detallado** y completo
- ‚úÖ Incluye **contexto** y **beneficios**
- ‚úÖ M√°s **persuasivo**
- ‚ö†Ô∏è Toma m√°s tiempo generar
- ‚ö†Ô∏è Usa m√°s recursos

**Casos de uso:**
- Descripciones de productos completas
- Art√≠culos de blog
- Documentaci√≥n
- Respuestas educativas
- An√°lisis detallados

---

### ‚ö†Ô∏è Consideraciones importantes:

1. **El modelo puede detenerse antes** si completa naturalmente la respuesta
2. **No es un l√≠mite estricto de palabras**, sino de tokens
3. **Afecta el costo computacional**: m√°s tokens = m√°s tiempo de procesamiento
4. **Debe considerar el contexto total**: prompt + respuesta no debe exceder `n_ctx`

### üìù C√≥digo de ejemplo:

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/llama-3-8b.gguf")

prompt = "Write a product description for a smartwatch."

# Respuesta corta
response_short = llm(
    prompt=prompt,
    max_tokens=20,
    temperature=0.7
)

# Respuesta detallada
response_long = llm(
    prompt=prompt,
    max_tokens=150,
    temperature=0.7
)

print("SHORT (20 tokens):")
print(response_short["choices"][0]["text"])
print("\n" + "="*60 + "\n")
print("LONG (150 tokens):")
print(response_long["choices"][0]["text"])
```

---

## üé® 8. Combinando par√°metros

La verdadera potencia viene de **combinar m√∫ltiples par√°metros** para lograr el resultado deseado.

### Ejemplo 1: Descripci√≥n concisa y factual de un auto el√©ctrico

**Objetivo:** Descripci√≥n corta, precisa y profesional

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/llama-3-8b.gguf")

prompt = "Describe an electric car."

response = llm(
    prompt=prompt,
    temperature=0.2,    # Baja creatividad, m√°s factual
    top_k=1,            # Solo palabras m√°s probables
    top_p=0.4,          # Enfoque en lo esencial
    max_tokens=20       # Respuesta muy corta
)

print(response["choices"][0]["text"])
```

**Resultado esperado:**

```
An electric vehicle powered by rechargeable batteries, 
offering zero emissions and efficient performance.
```

**An√°lisis de la configuraci√≥n:**

| Par√°metro | Valor | Efecto |
|-----------|-------|--------|
| `temperature=0.2` | Bajo | Respuesta predecible y factual |
| `top_k=1` | M√≠nimo | Solo la palabra m√°s probable |
| `top_p=0.4` | Bajo | Enfocado en lo esencial |
| `max_tokens=20` | Corto | Descripci√≥n concisa |

**Resultado:** Descripci√≥n **corta, precisa y profesional** ‚úÖ

---

### Ejemplo 2: Descripci√≥n creativa y detallada de un auto el√©ctrico

**Objetivo:** Descripci√≥n larga, atractiva y persuasiva (para marketing)

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/llama-3-8b.gguf")

prompt = "Describe an electric car."

response = llm(
    prompt=prompt,
    temperature=0.8,    # Alta creatividad
    top_k=10,           # M√°s opciones de palabras
    top_p=0.9,          # Amplio rango de vocabulario
    max_tokens=100      # Respuesta detallada
)

print(response["choices"][0]["text"])
```

**Resultado esperado:**

```
Experience the future of automotive innovation with this 
stunning electric vehicle! Seamlessly blending cutting-edge 
technology with environmental consciousness, this remarkable 
car delivers exhilarating instant torque, whisper-quiet 
operation, and zero tailpipe emissions. With its sleek, 
aerodynamic design and advanced regenerative braking system, 
you'll enjoy an impressive range of up to 300 miles on a 
single charge. The spacious, tech-forward interior features 
a state-of-the-art infotainment system, premium materials, 
and autonomous driving capabilities that redefine your 
journey. Embrace sustainable luxury and join the electric 
revolution today!
```

**An√°lisis de la configuraci√≥n:**

| Par√°metro | Valor | Efecto |
|-----------|-------|--------|
| `temperature=0.8` | Alto | Respuesta creativa y variada |
| `top_k=10` | Medio | Vocabulario m√°s rico |
| `top_p=0.9` | Alto | Amplia selecci√≥n de palabras |
| `max_tokens=100` | Largo | Descripci√≥n completa y detallada |

**Resultado:** Descripci√≥n **creativa, persuasiva y detallada** ‚úÖ

---

## üìã 9. Gu√≠a r√°pida de configuraciones recomendadas

### üéØ Para diferentes casos de uso:

#### 1. **Documentaci√≥n t√©cnica / Respuestas factuales**

```python
response = llm(
    prompt=prompt,
    temperature=0.1,
    top_k=1,
    top_p=0.5,
    max_tokens=200
)
```

**Caracter√≠sticas:** Preciso, consistente, factual

---

#### 2. **Res√∫menes ejecutivos**

```python
response = llm(
    prompt=prompt,
    temperature=0.3,
    top_k=5,
    top_p=0.6,
    max_tokens=100
)
```

**Caracter√≠sticas:** Conciso, claro, profesional

---

#### 3. **Contenido de marketing / Copywriting**

```python
response = llm(
    prompt=prompt,
    temperature=0.8,
    top_k=40,
    top_p=0.9,
    max_tokens=150
)
```

**Caracter√≠sticas:** Creativo, persuasivo, atractivo

---

#### 4. **Brainstorming / Ideas creativas**

```python
response = llm(
    prompt=prompt,
    temperature=0.9,
    top_k=50,
    top_p=0.95,
    max_tokens=200
)
```

**Caracter√≠sticas:** Muy creativo, diverso, innovador

---

#### 5. **Chatbot conversacional**

```python
response = llm(
    prompt=prompt,
    temperature=0.7,
    top_k=30,
    top_p=0.85,
    max_tokens=100
)
```

**Caracter√≠sticas:** Natural, balanceado, amigable

---

#### 6. **An√°lisis de datos / Reportes**

```python
response = llm(
    prompt=prompt,
    temperature=0.2,
    top_k=10,
    top_p=0.7,
    max_tokens=300
)
```

**Caracter√≠sticas:** Objetivo, estructurado, detallado

---

#### 7. **Generaci√≥n de c√≥digo**

```python
response = llm(
    prompt=prompt,
    temperature=0.1,
    top_k=1,
    top_p=0.5,
    max_tokens=500
)
```

**Caracter√≠sticas:** Preciso, sint√°cticamente correcto, predecible

---

#### 8. **Storytelling / Narrativa**

```python
response = llm(
    prompt=prompt,
    temperature=0.85,
    top_k=40,
    top_p=0.9,
    max_tokens=400
)
```

**Caracter√≠sticas:** Creativo, descriptivo, envolvente

---

## üß™ 10. Experimentaci√≥n y mejores pr√°cticas

### ‚úÖ Mejores pr√°cticas:

1. **Empieza con valores por defecto**
   ```python
   temperature=0.7
   top_k=40
   top_p=0.9
   max_tokens=256
   ```

2. **Ajusta un par√°metro a la vez**
   - Cambia solo uno para ver su efecto individual
   - Documenta los resultados

3. **Prueba con el mismo prompt**
   - Usa el mismo prompt para comparar resultados
   - Facilita identificar el impacto de cada par√°metro

4. **Considera el caso de uso**
   - ¬øNecesitas precisi√≥n o creatividad?
   - ¬øRespuesta corta o detallada?
   - ¬øTono formal o casual?

5. **Monitorea la calidad**
   - Revisa si las respuestas son coherentes
   - Verifica que no haya repeticiones excesivas
   - Aseg√∫rate de que la longitud sea apropiada

### ‚ö†Ô∏è Errores comunes a evitar:

‚ùå **Temperatura demasiado alta (>1.0)**
- Puede generar texto incoherente o sin sentido

‚ùå **Max tokens demasiado bajo**
- Puede cortar respuestas importantes a la mitad

‚ùå **Combinar valores extremos**
- Ej: `temperature=0.0` con `top_k=100` (contradictorios)

‚ùå **No considerar el contexto total**
- `max_tokens` + longitud del prompt debe caber en `n_ctx`

‚ùå **Usar los mismos par√°metros para todo**
- Diferentes tareas requieren diferentes configuraciones

---

## üìä 11. Tabla comparativa de efectos

| Par√°metro | Valor Bajo | Valor Alto |
|-----------|------------|------------|
| **Temperature** | Predecible, factual, repetitivo | Creativo, variado, arriesgado |
| **Top-k** | Conservador, limitado | Expresivo, diverso |
| **Top-p** | Enfocado, preciso | Variado, completo |
| **Max tokens** | Conciso, breve | Detallado, extenso |

---

## üíª 12. C√≥digo completo de ejemplo

```python
"""
Ejemplo completo: Comparaci√≥n de configuraciones de par√°metros
"""

from llama_cpp import Llama

def generate_description(llm, prompt, config_name, **params):
    """Genera una descripci√≥n con par√°metros espec√≠ficos"""
    print(f"\n{'='*70}")
    print(f"CONFIGURACI√ìN: {config_name}")
    print(f"{'='*70}")
    print(f"Par√°metros: {params}")
    print(f"{'-'*70}\n")
    
    response = llm(prompt=prompt, **params)
    text = response["choices"][0]["text"]
    
    print(f"Resultado:\n{text}\n")
    
    # Mostrar estad√≠sticas
    usage = response.get("usage", {})
    print(f"üìä Tokens generados: {usage.get('completion_tokens', 'N/A')}")
    print(f"{'='*70}\n")
    
    return text

def main():
    # Inicializar modelo
    print("Cargando modelo Llama 3...")
    llm = Llama(
        model_path="./models/llama-3-8b.gguf",
        n_ctx=2048,
        n_threads=8,
        verbose=False
    )
    print("‚úÖ Modelo cargado\n")
    
    # Prompt com√∫n
    prompt = "Describe an electric car."
    
    # Configuraci√≥n 1: Factual y conciso
    generate_description(
        llm, prompt,
        config_name="FACTUAL Y CONCISO",
        temperature=0.2,
        top_k=1,
        top_p=0.4,
        max_tokens=20
    )
    
    # Configuraci√≥n 2: Creativo y detallado
    generate_description(
        llm, prompt,
        config_name="CREATIVO Y DETALLADO",
        temperature=0.8,
        top_k=10,
        top_p=0.9,
        max_tokens=100
    )
    
    # Configuraci√≥n 3: Balanceado (uso general)
    generate_description(
        llm, prompt,
        config_name="BALANCEADO (RECOMENDADO)",
        temperature=0.7,
        top_k=40,
        top_p=0.85,
        max_tokens=80
    )
    
    # Configuraci√≥n 4: Muy creativo (brainstorming)
    generate_description(
        llm, prompt,
        config_name="MUY CREATIVO (BRAINSTORMING)",
        temperature=0.9,
        top_k=50,
        top_p=0.95,
        max_tokens=120
    )
    
    # Configuraci√≥n 5: Muy preciso (t√©cnico)
    generate_description(
        llm, prompt,
        config_name="MUY PRECISO (T√âCNICO)",
        temperature=0.1,
        top_k=1,
        top_p=0.5,
        max_tokens=150
    )

if __name__ == "__main__":
    main()
```

---

## üéì 13. Conceptos clave para recordar

### T√©rminos importantes:

- **Decoding parameters:** Par√°metros que controlan c√≥mo se transforma la salida del modelo en texto
- **Temperature:** Controla la aleatoriedad/creatividad (0 = predecible, 1 = creativo)
- **Top-k:** Limita cu√°ntas palabras considera el modelo en cada paso
- **Top-p (Nucleus sampling):** Limita palabras por probabilidad acumulada
- **Max tokens:** N√∫mero m√°ximo de tokens a generar
- **Token:** Unidad b√°sica de texto (‚âà0.75 palabras en ingl√©s)

### Reglas de oro:

1. **No existe una configuraci√≥n perfecta universal**
   - Cada tarea requiere ajustes espec√≠ficos

2. **Experimenta y documenta**
   - Prueba diferentes combinaciones
   - Guarda las que funcionan bien

3. **Empieza conservador, ajusta gradualmente**
   - Comienza con valores moderados
   - Aumenta/disminuye seg√∫n necesites

4. **Considera el trade-off**
   - Creatividad vs Precisi√≥n
   - Longitud vs Concisi√≥n
   - Velocidad vs Calidad

---

## üöÄ 14. Pr√≥ximos pasos

Despu√©s de dominar los par√°metros b√°sicos, puedes:

1. **Explorar par√°metros avanzados**
   - `repeat_penalty`: Penaliza repeticiones
   - `frequency_penalty`: Reduce palabras frecuentes
   - `presence_penalty`: Fomenta nuevos temas
   - `stop`: Define secuencias de parada personalizadas

2. **Crear presets personalizados**
   - Guarda configuraciones para diferentes tareas
   - Crea una biblioteca de configuraciones

3. **Implementar A/B testing**
   - Compara diferentes configuraciones
   - Mide cu√°l funciona mejor para tu caso

4. **Automatizar la selecci√≥n de par√°metros**
   - Crea funciones que elijan par√°metros seg√∫n el tipo de tarea
   - Implementa l√≥gica adaptativa

---

## üìö 15. Recursos adicionales

### Documentaci√≥n:
- [llama-cpp-python - Par√°metros de generaci√≥n](https://github.com/abetlen/llama-cpp-python)
- [Hugging Face - Generation Parameters](https://huggingface.co/docs/transformers/main_classes/text_generation)

### Art√≠culos recomendados:
- "The Curious Case of Neural Text Degeneration" (sobre nucleus sampling)
- "Temperature in Language Models" (an√°lisis profundo)

### Herramientas √∫tiles:
- [LLM Parameter Playground](https://huggingface.co/spaces) - Experimenta visualmente
- Notebooks de Jupyter para experimentaci√≥n

---

## üí° 16. Resumen ejecutivo

Los **par√°metros de decodificaci√≥n** te permiten controlar c√≥mo Llama 3 genera texto:

| Par√°metro | Qu√© controla | Valores t√≠picos |
|-----------|--------------|-----------------|
| **temperature** | Creatividad vs Precisi√≥n | 0.1 (preciso) - 0.9 (creativo) |
| **top_k** | N√∫mero de palabras a considerar | 1 (limitado) - 50 (variado) |
| **top_p** | Probabilidad acumulada | 0.4 (enfocado) - 0.9 (diverso) |
| **max_tokens** | Longitud de respuesta | 20 (corto) - 500+ (largo) |

**Configuraci√≥n recomendada para empezar:**
```python
temperature=0.7, top_k=40, top_p=0.9, max_tokens=256
```

**Ajusta seg√∫n necesites:**
- ‚¨áÔ∏è Baja temperatura/top-k/top-p para **precisi√≥n**
- ‚¨ÜÔ∏è Sube temperatura/top-k/top-p para **creatividad**
- ‚¨áÔ∏è Baja max_tokens para **concisi√≥n**
- ‚¨ÜÔ∏è Sube max_tokens para **detalle**

---

## ‚ú® Conclusi√≥n

Dominar los par√°metros de Llama 3 es como aprender a afinar un instrumento musical: con pr√°ctica y experimentaci√≥n, puedes lograr exactamente el tono y estilo que necesitas para cada situaci√≥n. No tengas miedo de experimentar y encontrar las configuraciones que mejor funcionen para tus casos de uso espec√≠ficos.

**¬°Ahora est√°s listo para ajustar Llama 3 como un profesional! üéØ**

---

*Notas generadas a partir del curso "Working with Llama 3"*  
*Lecci√≥n: Tuning Llama 3 Parameters*  
*XP de la lecci√≥n: 50 | XP del d√≠a: 750*