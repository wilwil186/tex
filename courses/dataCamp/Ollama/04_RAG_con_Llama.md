# Notas Completas: RAG con Llama 3

## ğŸ“š Curso: Working with Llama 3
**Tema:** Retrieval Augmented Generation (RAG)  
**Nivel:** Intermedio-Avanzado

---

## ğŸ¯ 1. Â¿QuÃ© es RAG?

### DefiniciÃ³n:

**RAG (Retrieval Augmented Generation)** es una tÃ©cnica que combina:
- **RecuperaciÃ³n de informaciÃ³n** (Retrieval) de una base de conocimiento
- **GeneraciÃ³n de texto** (Generation) usando un LLM

### El problema que resuelve:

Los LLMs tienen limitaciones:
- âŒ **Conocimiento limitado** al momento de entrenamiento
- âŒ **No tienen acceso** a informaciÃ³n privada/especÃ­fica
- âŒ **Pueden "alucinar"** informaciÃ³n incorrecta
- âŒ **No pueden actualizar** su conocimiento fÃ¡cilmente

### La soluciÃ³n RAG:

âœ… **Proporciona contexto relevante** al LLM antes de generar  
âœ… **Accede a informaciÃ³n actualizada** de bases de datos  
âœ… **Reduce alucinaciones** con datos verificables  
âœ… **Permite conocimiento especÃ­fico** de tu dominio  

### AnalogÃ­a:

Imagina que estÃ¡s en un examen:
- **Sin RAG:** Respondes solo con lo que memorizaste
- **Con RAG:** Puedes consultar libros y apuntes antes de responder

---

## ğŸ”„ 2. Arquitectura de RAG

### Flujo bÃ¡sico:

```
1. Usuario hace una pregunta
   â†“
2. Sistema busca informaciÃ³n relevante en base de datos
   â†“
3. Recupera documentos/fragmentos mÃ¡s relevantes
   â†“
4. Combina pregunta + contexto recuperado
   â†“
5. LLM genera respuesta basada en el contexto
   â†“
6. Usuario recibe respuesta fundamentada
```

### Componentes principales:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SISTEMA RAG                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Documentos  â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚  Embeddings     â”‚ â”‚
â”‚  â”‚  (Fuente)    â”‚      â”‚  (Vectores)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚              â”‚
â”‚                              â–¼              â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  Vector Store   â”‚  â”‚
â”‚  â”‚   Pregunta   â”‚     â”‚  (Base de datos)â”‚  â”‚
â”‚  â”‚   Usuario    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚            â”‚
â”‚         â”‚                      â”‚            â”‚
â”‚         â–¼                      â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    BÃºsqueda de Similitud            â”‚   â”‚
â”‚  â”‚    (RecuperaciÃ³n)                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                   â”‚                         â”‚
â”‚                   â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Contexto + Pregunta â†’ LLM          â”‚   â”‚
â”‚  â”‚  (GeneraciÃ³n)                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                   â”‚                         â”‚
â”‚                   â–¼                         â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚            â”‚   Respuesta  â”‚                 â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§® 3. Embeddings - La base de RAG

### Â¿QuÃ© son los embeddings?

**Embeddings** son representaciones numÃ©ricas (vectores) de texto que capturan su significado semÃ¡ntico.

### Ejemplo visual:

```
Texto: "El gato duerme"
Embedding: [0.23, -0.45, 0.67, 0.12, ..., 0.89]
           (vector de 384 o mÃ¡s dimensiones)

Texto similar: "El felino descansa"
Embedding: [0.25, -0.43, 0.65, 0.15, ..., 0.87]
           (vector cercano en el espacio)
```

### Propiedad clave:

**Textos con significados similares tienen embeddings cercanos en el espacio vectorial.**

### Modelos de embeddings populares:

| Modelo | Dimensiones | TamaÃ±o | Uso |
|--------|-------------|--------|-----|
| **sentence-transformers/all-MiniLM-L6-v2** | 384 | 80 MB | General, rÃ¡pido |
| **sentence-transformers/all-mpnet-base-v2** | 768 | 420 MB | Alta calidad |
| **text-embedding-ada-002** (OpenAI) | 1536 | API | Muy preciso |
| **BAAI/bge-small-en-v1.5** | 384 | 133 MB | Eficiente |

---

## ğŸ› ï¸ 4. ImplementaciÃ³n bÃ¡sica de RAG

### 4.1 InstalaciÃ³n de dependencias:

```bash
pip install ollama
pip install sentence-transformers
pip install chromadb
pip install langchain
pip install langchain-community
```

### 4.2 CÃ³digo completo - RAG simple:

```python
import ollama
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

class SimpleRAG:
    def __init__(self, model_name='llama3'):
        self.llm_model = model_name
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        self.client = chromadb.Client(Settings(
            anonymized_telemetry=False
        ))
        
        self.collection = self.client.create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}
        )
        
        print("âœ… RAG System inicializado")
    
    def add_documents(self, documents):
        """AÃ±ade documentos a la base de conocimiento"""
        print(f"AÃ±adiendo {len(documents)} documentos...")
        
        embeddings = self.embedding_model.encode(documents).tolist()
        
        ids = [f"doc_{i}" for i in range(len(documents))]
        
        self.collection.add(
            embeddings=embeddings,
            documents=documents,
            ids=ids
        )
        
        print(f"âœ… {len(documents)} documentos aÃ±adidos")
    
    def retrieve(self, query, n_results=3):
        """Recupera documentos relevantes"""
        query_embedding = self.embedding_model.encode([query]).tolist()
        
        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=n_results
        )
        
        return results['documents'][0]
    
    def generate(self, query, context):
        """Genera respuesta usando LLM"""
        prompt = f"""
Usa el siguiente contexto para responder la pregunta.
Si no puedes responder con el contexto dado, di que no tienes suficiente informaciÃ³n.

Contexto:
{context}

Pregunta: {query}

Respuesta:
"""
        
        response = ollama.generate(
            model=self.llm_model,
            prompt=prompt,
            options={
                'temperature': 0.3,
                'num_predict': 200
            }
        )
        
        return response['response']
    
    def query(self, question, n_results=3):
        """Pipeline completo RAG"""
        print(f"\nğŸ” Pregunta: {question}")
        
        relevant_docs = self.retrieve(question, n_results)
        
        print(f"ğŸ“š Documentos recuperados: {len(relevant_docs)}")
        for i, doc in enumerate(relevant_docs, 1):
            print(f"  {i}. {doc[:100]}...")
        
        context = "\n\n".join(relevant_docs)
        
        print("\nğŸ¤– Generando respuesta...")
        answer = self.generate(question, context)
        
        return answer

if __name__ == "__main__":
    rag = SimpleRAG()
    
    documents = [
        "Python es un lenguaje de programaciÃ³n de alto nivel, interpretado y de propÃ³sito general.",
        "Python fue creado por Guido van Rossum y lanzado por primera vez en 1991.",
        "Python usa indentaciÃ³n para definir bloques de cÃ³digo en lugar de llaves.",
        "Las listas en Python son estructuras de datos mutables que pueden contener elementos de diferentes tipos.",
        "Los diccionarios en Python son colecciones no ordenadas de pares clave-valor.",
        "NumPy es una biblioteca de Python para computaciÃ³n cientÃ­fica que proporciona arrays multidimensionales.",
        "Pandas es una biblioteca de Python para anÃ¡lisis y manipulaciÃ³n de datos.",
        "Django es un framework web de alto nivel escrito en Python.",
        "Flask es un microframework web minimalista para Python.",
        "Machine learning en Python se facilita con bibliotecas como scikit-learn y TensorFlow."
    ]
    
    rag.add_documents(documents)
    
    questions = [
        "Â¿QuiÃ©n creÃ³ Python?",
        "Â¿QuÃ© es NumPy?",
        "Â¿CÃ³mo define Python los bloques de cÃ³digo?"
    ]
    
    for question in questions:
        answer = rag.query(question)
        print(f"\nğŸ’¡ Respuesta: {answer}")
        print("="*60)
```

---

## ğŸ“Š 5. RAG con ChromaDB (Avanzado)

### 5.1 Â¿QuÃ© es ChromaDB?

**ChromaDB** es una base de datos vectorial diseÃ±ada especÃ­ficamente para embeddings y RAG.

**CaracterÃ­sticas:**
- âœ… Almacenamiento persistente
- âœ… BÃºsqueda de similitud eficiente
- âœ… Filtrado por metadatos
- âœ… FÃ¡cil integraciÃ³n con Python

### 5.2 ImplementaciÃ³n con persistencia:

```python
import ollama
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import os

class PersistentRAG:
    def __init__(self, persist_directory="./chroma_db", model_name='llama3'):
        self.llm_model = model_name
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        os.makedirs(persist_directory, exist_ok=True)
        
        self.client = chromadb.PersistentClient(path=persist_directory)
        
        try:
            self.collection = self.client.get_collection("documents")
            print("âœ… ColecciÃ³n existente cargada")
        except:
            self.collection = self.client.create_collection(
                name="documents",
                metadata={"hnsw:space": "cosine"}
            )
            print("âœ… Nueva colecciÃ³n creada")
    
    def add_documents(self, documents, metadatas=None):
        """AÃ±ade documentos con metadatos opcionales"""
        embeddings = self.embedding_model.encode(documents).tolist()
        
        existing_count = self.collection.count()
        ids = [f"doc_{existing_count + i}" for i in range(len(documents))]
        
        if metadatas is None:
            metadatas = [{"source": "manual"} for _ in documents]
        
        self.collection.add(
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        
        print(f"âœ… {len(documents)} documentos aÃ±adidos (Total: {self.collection.count()})")
    
    def retrieve_with_metadata(self, query, n_results=3, filter_dict=None):
        """Recupera documentos con filtrado por metadatos"""
        query_embedding = self.embedding_model.encode([query]).tolist()
        
        query_params = {
            'query_embeddings': query_embedding,
            'n_results': n_results
        }
        
        if filter_dict:
            query_params['where'] = filter_dict
        
        results = self.collection.query(**query_params)
        
        return {
            'documents': results['documents'][0],
            'metadatas': results['metadatas'][0],
            'distances': results['distances'][0]
        }
    
    def query_with_sources(self, question, n_results=3):
        """Query con informaciÃ³n de fuentes"""
        print(f"\nğŸ” Pregunta: {question}")
        
        results = self.retrieve_with_metadata(question, n_results)
        
        print(f"\nğŸ“š Documentos recuperados:")
        for i, (doc, meta, dist) in enumerate(zip(
            results['documents'],
            results['metadatas'],
            results['distances']
        ), 1):
            print(f"  {i}. [Similitud: {1-dist:.3f}] {doc[:80]}...")
            print(f"     Fuente: {meta.get('source', 'N/A')}")
        
        context = "\n\n".join([
            f"[Fuente: {meta.get('source', 'N/A')}]\n{doc}"
            for doc, meta in zip(results['documents'], results['metadatas'])
        ])
        
        prompt = f"""
Usa el siguiente contexto para responder la pregunta.
Menciona las fuentes cuando sea relevante.

Contexto:
{context}

Pregunta: {question}

Respuesta:
"""
        
        response = ollama.generate(
            model=self.llm_model,
            prompt=prompt,
            options={'temperature': 0.3}
        )
        
        return response['response']
    
    def get_stats(self):
        """Obtiene estadÃ­sticas de la base de datos"""
        count = self.collection.count()
        return {
            'total_documents': count,
            'collection_name': self.collection.name
        }

if __name__ == "__main__":
    rag = PersistentRAG()
    
    documents = [
        "La fotosÃ­ntesis es el proceso por el cual las plantas convierten luz solar en energÃ­a quÃ­mica.",
        "El ADN contiene las instrucciones genÃ©ticas para el desarrollo y funcionamiento de los organismos.",
        "La mitosis es el proceso de divisiÃ³n celular que resulta en dos cÃ©lulas hijas idÃ©nticas.",
        "Los ecosistemas son comunidades de organismos que interactÃºan con su entorno fÃ­sico.",
        "La evoluciÃ³n es el cambio en las caracterÃ­sticas heredables de poblaciones a lo largo del tiempo."
    ]
    
    metadatas = [
        {"source": "BiologÃ­a - CapÃ­tulo 3", "topic": "fotosÃ­ntesis"},
        {"source": "GenÃ©tica - CapÃ­tulo 1", "topic": "ADN"},
        {"source": "BiologÃ­a Celular - CapÃ­tulo 5", "topic": "divisiÃ³n celular"},
        {"source": "EcologÃ­a - CapÃ­tulo 2", "topic": "ecosistemas"},
        {"source": "EvoluciÃ³n - CapÃ­tulo 1", "topic": "evoluciÃ³n"}
    ]
    
    rag.add_documents(documents, metadatas)
    
    stats = rag.get_stats()
    print(f"\nğŸ“Š EstadÃ­sticas: {stats}")
    
    answer = rag.query_with_sources("Â¿QuÃ© es la fotosÃ­ntesis?")
    print(f"\nğŸ’¡ Respuesta:\n{answer}")
```

---

## ğŸ”— 6. RAG con LangChain

### 6.1 Â¿Por quÃ© LangChain?

**LangChain** es un framework que simplifica la construcciÃ³n de aplicaciones con LLMs.

**Ventajas:**
- âœ… Abstracciones de alto nivel
- âœ… Componentes reutilizables
- âœ… IntegraciÃ³n con mÃºltiples LLMs y vectorstores
- âœ… Cadenas (chains) predefinidas para RAG

### 6.2 ImplementaciÃ³n con LangChain:

```python
from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

class LangChainRAG:
    def __init__(self, model_name='llama3', persist_directory="./langchain_db"):
        print("Inicializando LangChain RAG...")
        
        self.llm = Ollama(
            model=model_name,
            temperature=0.3
        )
        
        self.embeddings = HuggingFaceEmbeddings(
            model_name='sentence-transformers/all-MiniLM-L6-v2'
        )
        
        self.persist_directory = persist_directory
        self.vectorstore = None
        self.qa_chain = None
        
        print("âœ… LangChain RAG inicializado")
    
    def load_documents(self, texts):
        """Carga y procesa documentos"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            length_function=len
        )
        
        chunks = []
        for text in texts:
            chunks.extend(text_splitter.split_text(text))
        
        print(f"ğŸ“„ {len(texts)} documentos divididos en {len(chunks)} chunks")
        
        self.vectorstore = Chroma.from_texts(
            texts=chunks,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )
        
        self._create_qa_chain()
        
        print("âœ… Documentos cargados en vectorstore")
    
    def _create_qa_chain(self):
        """Crea la cadena de QA"""
        template = """
Usa el siguiente contexto para responder la pregunta.
Si no sabes la respuesta, di que no tienes suficiente informaciÃ³n.
No inventes informaciÃ³n.

Contexto: {context}

Pregunta: {question}

Respuesta Ãºtil:
"""
        
        prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
        
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 3}
            ),
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )
    
    def query(self, question):
        """Realiza una consulta"""
        if self.qa_chain is None:
            return "Error: No hay documentos cargados"
        
        print(f"\nğŸ” Pregunta: {question}")
        
        result = self.qa_chain.invoke({"query": question})
        
        answer = result['result']
        sources = result['source_documents']
        
        print(f"\nğŸ“š Fuentes consultadas: {len(sources)}")
        for i, doc in enumerate(sources, 1):
            print(f"  {i}. {doc.page_content[:100]}...")
        
        return answer

if __name__ == "__main__":
    rag = LangChainRAG()
    
    documents = [
        """
        Python es un lenguaje de programaciÃ³n interpretado de alto nivel.
        Fue creado por Guido van Rossum y lanzado en 1991.
        Python enfatiza la legibilidad del cÃ³digo y usa indentaciÃ³n significativa.
        Es ampliamente usado en desarrollo web, ciencia de datos, IA y automatizaciÃ³n.
        """,
        """
        NumPy es la biblioteca fundamental para computaciÃ³n cientÃ­fica en Python.
        Proporciona soporte para arrays multidimensionales y matrices.
        Incluye funciones matemÃ¡ticas de alto nivel para operar con estos arrays.
        NumPy es la base de muchas otras bibliotecas cientÃ­ficas de Python.
        """,
        """
        Pandas es una biblioteca de Python para anÃ¡lisis y manipulaciÃ³n de datos.
        Proporciona estructuras de datos como DataFrame y Series.
        Facilita la limpieza, transformaciÃ³n y anÃ¡lisis de datos tabulares.
        Es esencial para ciencia de datos y anÃ¡lisis de datos en Python.
        """
    ]
    
    rag.load_documents(documents)
    
    questions = [
        "Â¿QuiÃ©n creÃ³ Python y cuÃ¡ndo?",
        "Â¿Para quÃ© se usa NumPy?",
        "Â¿QuÃ© estructuras de datos proporciona Pandas?"
    ]
    
    for question in questions:
        answer = rag.query(question)
        print(f"\nğŸ’¡ Respuesta: {answer}")
        print("="*60)
```

---

## ğŸ“„ 7. RAG con documentos PDF

### 7.1 InstalaciÃ³n adicional:

```bash
pip install pypdf
pip install langchain-community
```

### 7.2 Procesamiento de PDFs:

```python
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
import os

class PDFRAG:
    def __init__(self, model_name='llama3'):
        self.llm = Ollama(model=model_name, temperature=0.3)
        self.embeddings = HuggingFaceEmbeddings(
            model_name='sentence-transformers/all-MiniLM-L6-v2'
        )
        self.vectorstore = None
        self.qa_chain = None
    
    def load_pdf(self, pdf_path):
        """Carga y procesa un PDF"""
        print(f"ğŸ“„ Cargando PDF: {pdf_path}")
        
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"PDF no encontrado: {pdf_path}")
        
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()
        
        print(f"âœ… {len(pages)} pÃ¡ginas cargadas")
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        
        chunks = text_splitter.split_documents(pages)
        print(f"ğŸ“ Dividido en {len(chunks)} chunks")
        
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory="./pdf_db"
        )
        
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 4}),
            return_source_documents=True
        )
        
        print("âœ… PDF procesado y listo para consultas")
    
    def load_multiple_pdfs(self, pdf_directory):
        """Carga mÃºltiples PDFs de un directorio"""
        pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]
        
        print(f"ğŸ“š Encontrados {len(pdf_files)} PDFs")
        
        all_chunks = []
        
        for pdf_file in pdf_files:
            pdf_path = os.path.join(pdf_directory, pdf_file)
            print(f"  Procesando: {pdf_file}")
            
            loader = PyPDFLoader(pdf_path)
            pages = loader.load()
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            
            chunks = text_splitter.split_documents(pages)
            all_chunks.extend(chunks)
        
        print(f"âœ… Total de chunks: {len(all_chunks)}")
        
        self.vectorstore = Chroma.from_documents(
            documents=all_chunks,
            embedding=self.embeddings,
            persist_directory="./multi_pdf_db"
        )
        
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 5}),
            return_source_documents=True
        )
        
        print("âœ… Todos los PDFs procesados")
    
    def query(self, question):
        """Consulta el sistema RAG"""
        if self.qa_chain is None:
            return "Error: No hay documentos cargados"
        
        print(f"\nğŸ” Pregunta: {question}")
        
        result = self.qa_chain.invoke({"query": question})
        
        answer = result['result']
        sources = result['source_documents']
        
        print(f"\nğŸ“š Fuentes:")
        for i, doc in enumerate(sources, 1):
            page = doc.metadata.get('page', 'N/A')
            source = doc.metadata.get('source', 'N/A')
            print(f"  {i}. PÃ¡gina {page} de {os.path.basename(source)}")
            print(f"     {doc.page_content[:150]}...")
        
        return answer

if __name__ == "__main__":
    rag = PDFRAG()
    
    rag.load_pdf("documento.pdf")
    
    answer = rag.query("Â¿CuÃ¡l es el tema principal del documento?")
    print(f"\nğŸ’¡ Respuesta:\n{answer}")
```

---

## ğŸ¯ 8. Mejores prÃ¡cticas para RAG

### 8.1 Chunking (DivisiÃ³n de documentos):

**Estrategias:**

1. **Fixed-size chunking:**
```python
chunk_size = 500
chunk_overlap = 50
```

2. **Semantic chunking:**
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " ", ""]
)
```

3. **Document-aware chunking:**
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    is_separator_regex=False
)
```

### 8.2 OptimizaciÃ³n de retrieval:

```python
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,
        "fetch_k": 20,
        "lambda_mult": 0.5
    }
)
```

**ParÃ¡metros:**
- `k`: NÃºmero de documentos a devolver
- `fetch_k`: NÃºmero de documentos a recuperar antes de filtrar
- `lambda_mult`: Balance entre relevancia y diversidad (0-1)

### 8.3 Prompt engineering para RAG:

```python
template = """
Eres un asistente experto que responde preguntas basÃ¡ndose en el contexto proporcionado.

REGLAS:
1. Usa SOLO la informaciÃ³n del contexto para responder
2. Si el contexto no contiene la respuesta, di "No tengo suficiente informaciÃ³n"
3. Cita las fuentes cuando sea posible
4. SÃ© conciso pero completo
5. No inventes informaciÃ³n

Contexto:
{context}

Pregunta: {question}

Respuesta detallada:
"""
```

### 8.4 EvaluaciÃ³n de RAG:

```python
def evaluate_rag(rag_system, test_questions, expected_answers):
    """EvalÃºa el sistema RAG"""
    results = []
    
    for question, expected in zip(test_questions, expected_answers):
        answer = rag_system.query(question)
        
        similarity = calculate_similarity(answer, expected)
        
        results.append({
            'question': question,
            'answer': answer,
            'expected': expected,
            'similarity': similarity
        })
    
    avg_similarity = sum(r['similarity'] for r in results) / len(results)
    
    return {
        'results': results,
        'average_similarity': avg_similarity
    }
```

---

## ğŸš€ 9. RAG avanzado - TÃ©cnicas adicionales

### 9.1 Re-ranking:

```python
from sentence_transformers import CrossEncoder

class ReRankingRAG:
    def __init__(self):
        self.retriever = ...
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    def retrieve_and_rerank(self, query, k=10, top_n=3):
        initial_docs = self.retriever.get_relevant_documents(query, k=k)
        
        pairs = [[query, doc.page_content] for doc in initial_docs]
        scores = self.reranker.predict(pairs)
        
        ranked_docs = sorted(
            zip(initial_docs, scores),
            key=lambda x: x[1],
            reverse=True
        )
        
        return [doc for doc, score in ranked_docs[:top_n]]
```

### 9.2 Hybrid Search (Keyword + Semantic):

```python
from langchain.retrievers import BM25Retriever, EnsembleRetriever

class HybridRAG:
    def __init__(self, documents):
        self.bm25_retriever = BM25Retriever.from_documents(documents)
        self.bm25_retriever.k = 5
        
        vectorstore = Chroma.from_documents(documents, embeddings)
        self.semantic_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[self.bm25_retriever, self.semantic_retriever],
            weights=[0.5, 0.5]
        )
    
    def retrieve(self, query):
        return self.ensemble_retriever.get_relevant_documents(query)
```

### 9.3 Query Expansion:

```python
def expand_query(original_query, llm):
    """Expande la query para mejor retrieval"""
    prompt = f"""
    Genera 3 variaciones de la siguiente pregunta que mantengan el mismo significado:
    
    Pregunta original: {original_query}
    
    Variaciones:
    1.
    2.
    3.
    """
    
    response = llm.generate(prompt)
    variations = parse_variations(response)
    
    return [original_query] + variations
```

---

## ğŸ“Š 10. Monitoreo y debugging de RAG

### 10.1 Logging detallado:

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DebugRAG:
    def query(self, question):
        logger.info(f"Query recibida: {question}")
        
        retrieved_docs = self.retrieve(question)
        logger.info(f"Documentos recuperados: {len(retrieved_docs)}")
        
        for i, doc in enumerate(retrieved_docs):
            logger.debug(f"Doc {i}: {doc[:100]}...")
        
        answer = self.generate(question, retrieved_docs)
        logger.info(f"Respuesta generada: {len(answer)} caracteres")
        
        return answer
```

### 10.2 MÃ©tricas de rendimiento:

```python
import time

class MetricsRAG:
    def __init__(self):
        self.metrics = {
            'retrieval_times': [],
            'generation_times': [],
            'total_queries': 0
        }
    
    def query_with_metrics(self, question):
        start_total = time.time()
        
        start_retrieval = time.time()
        docs = self.retrieve(question)
        retrieval_time = time.time() - start_retrieval
        
        start_generation = time.time()
        answer = self.generate(question, docs)
        generation_time = time.time() - start_generation
        
        total_time = time.time() - start_total
        
        self.metrics['retrieval_times'].append(retrieval_time)
        self.metrics['generation_times'].append(generation_time)
        self.metrics['total_queries'] += 1
        
        return answer, {
            'retrieval_time': retrieval_time,
            'generation_time': generation_time,
            'total_time': total_time
        }
    
    def get_average_metrics(self):
        return {
            'avg_retrieval_time': sum(self.metrics['retrieval_times']) / len(self.metrics['retrieval_times']),
            'avg_generation_time': sum(self.metrics['generation_times']) / len(self.metrics['generation_times']),
            'total_queries': self.metrics['total_queries']
        }
```

---

## ğŸ“ 11. Resumen ejecutivo

**RAG (Retrieval Augmented Generation)** combina bÃºsqueda de informaciÃ³n con generaciÃ³n de texto para:

âœ… **Proporcionar respuestas fundamentadas** en datos reales  
âœ… **Reducir alucinaciones** del LLM  
âœ… **Acceder a informaciÃ³n actualizada** y especÃ­fica  
âœ… **Mantener privacidad** procesando datos localmente  

**Componentes clave:**
1. **Embeddings:** Representaciones vectoriales de texto
2. **Vector Store:** Base de datos para bÃºsqueda de similitud
3. **Retriever:** Sistema de recuperaciÃ³n de documentos relevantes
4. **LLM:** Generador de respuestas basadas en contexto

**Stack tecnolÃ³gico recomendado:**
```python
ollama
sentence-transformers
chromadb
langchain
```

**Pipeline bÃ¡sico:**
```
Pregunta â†’ Embedding â†’ BÃºsqueda â†’ Contexto â†’ LLM â†’ Respuesta
```

---

## âœ¨ ConclusiÃ³n

RAG transforma los LLMs de generadores de texto a sistemas de conocimiento fundamentados en datos reales. Al combinar Llama 3 con RAG, puedes crear asistentes inteligentes que responden con precisiÃ³n basÃ¡ndose en tu propia base de conocimiento, manteniendo privacidad y control total sobre los datos.

---

*Notas generadas para el curso "Working with Llama 3"*  
*Tema: Retrieval Augmented Generation (RAG)*
